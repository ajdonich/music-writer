{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# This for managing relative imports from nb\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "    \n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re, mido\n",
    "from mido import MidiFile\n",
    "\n",
    "from mwriter.musicfactory import MusicFactory\n",
    "from mwriter.abstractfactory import MLDataSet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Just supress \"WavFileWarning: Chunk (non-data) not understood, skipping it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_BPM = 120\n",
    "FACTORY = MusicFactory()\n",
    "\n",
    "def get_note_strike_times(midi_file):\n",
    "#{\n",
    "    tempo = mido.bpm2tempo(DEFAULT_BPM)\n",
    "    \n",
    "    assert len(midi_file.tracks) == 1, f\"MIDI must have 1 track, not: {len(midi_file.tracks)}\"\n",
    " \n",
    "    strike_times = []\n",
    "    accrued_ticks = 0\n",
    "    for message in midi_file.tracks[0]: \n",
    "    #{\n",
    "        # Non-zero velocity --> note has been struck\n",
    "        # Zero velocity --> note has been released\n",
    "    \n",
    "        accrued_ticks += message.time\n",
    "        if message.type == 'note_on' and message.velocity > 0:\n",
    "            strike_times.append(mido.tick2second(accrued_ticks, midi_file.ticks_per_beat, tempo))\n",
    "    #}\n",
    "    \n",
    "    print(f\"Found {len(strike_times)} note strikes in MIDI file: {midi_file.filename}\")\n",
    "    \n",
    "    return strike_times\n",
    "#}\n",
    "\n",
    "def generate_contiguous_labels(midi_filename, m, sec_per_step):\n",
    "#{  \n",
    "    Y = np.zeros((m,))\n",
    "    strike_times = get_note_strike_times(midi_filename)\n",
    "    for strike_sec in strike_times: Y[int(strike_sec/sec_per_step)] = 1\n",
    "\n",
    "    return Y\n",
    "#}\n",
    "\n",
    "def generate_windowed_labels(midi_filename, m, time_batch, sec_per_step, stride=1):\n",
    "#{\n",
    "    # Pad an extra time_batch worth of zeros at the end of contiguous\n",
    "    Y_contiguous = generate_contiguous_labels(midi_filename, m+time_batch, sec_per_step)\n",
    "    \n",
    "    strided_steps = np.arange(0, m, stride)\n",
    "    Y_windowed = np.zeros((len(strided_steps), time_batch))\n",
    "    \n",
    "    for i in range(len(strided_steps)): \n",
    "        Y_windowed[i,:] = Y_contiguous[strided_steps[i]:strided_steps[i]+time_batch]\n",
    "\n",
    "    return Y_windowed\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read wav files (for now discard the 2nd stereo track).\n",
    "rate_train, data_train = wavfile.read('../data/audio/88_Key_Ascending_Chromatic_Scale.wav')\n",
    "rate_val, data_val = wavfile.read('../data/audio/88_Key_Descending_Chromatic_Scale.wav')\n",
    "data_train = data_train[:,0]; data_val = data_val[:,0]       \n",
    "\n",
    "assert rate_train == rate_val, f\"rate_train: {rate_train} must equal rate_val: {rate_val}\"\n",
    "\n",
    "# Init NFFT for the minimum piano semitone:\n",
    "# b/w A#0 and A0 (29.135 - 27.5 = 1.635Hz)\n",
    "NFFT = int(rate_train / 1.635)\n",
    "FREQ_RESOLUTION = rate_train / NFFT\n",
    "overlap = int(NFFT/2) \n",
    "\n",
    "# Perform FFTs, window of interest: principal piano freqs: A0 to C8 (27.5 to 4186.0 Hz)\n",
    "spectral_train, fft_imin_train, fft_imax_train, sec_per_step_train = FACTORY.\\\n",
    "    perform_fft(data_train, rate_train, NFFT, overlap, freq_window=(27.5, 4186.0))\n",
    "\n",
    "spectral_val, fft_imin_val, fft_imax_val, sec_per_step_val = FACTORY.\\\n",
    "    perform_fft(data_val, rate_val, NFFT, overlap, freq_window=(27.5, 4186.0))\n",
    "\n",
    "# Network inputs\n",
    "m = len(spectral_train)                    # Number of time_batch windowed samples\n",
    "n_freq = fft_imax_train - fft_imin_train   # Number of frequencies per sample (over window)\n",
    "time_batch = 1                             # ~3 second time window (10 == 307.46ms)\n",
    "\n",
    "# Reshape FFT data for DenseNet\n",
    "X_train = FACTORY.format_spectral_data(spectral_train, time_batch, (fft_imin_train, fft_imax_train))\n",
    "X_val = FACTORY.format_spectral_data(spectral_val, time_batch, (fft_imin_val, fft_imax_val))\n",
    "print(f\"Input data shape, train: {X_train.shape}, validate: {X_val.shape}\")\n",
    "\n",
    "# Network ouput will be a time_batch set of binary outs:\n",
    "# equal True if note struck on that step, False otherwise\n",
    "\n",
    "# Generate matching label dataset\n",
    "midi_train = MidiFile('../data/midi_cleaned/88_Key_Ascending_Chromatic_Scale.mid')\n",
    "midi_val = MidiFile('../data/midi_cleaned/88_Key_Descending_Chromatic_Scale.mid')\n",
    "\n",
    "Y_train = generate_contiguous_labels(midi_train, m, sec_per_step_train)\n",
    "Y_val = generate_contiguous_labels(midi_val, m, sec_per_step_val)\n",
    "print(f\"Output data shape, train: {Y_train.shape}, validate: {Y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_power_spikes(signal_list):\n",
    "#{\n",
    "    spikes = []; prev_slope = 1\n",
    "    for p in range(0, len(signal_list)-1):\n",
    "    #{            \n",
    "        slope = signal_list[p+1] - signal_list[p]\n",
    "        if (slope < 0) and (prev_slope > 0): spikes.append(1)\n",
    "        else: spikes.append(0)\n",
    "        prev_slope = slope\n",
    "    #}\n",
    "    \n",
    "    spikes.append(0)\n",
    "    return np.asarray(spikes)\n",
    "#}\n",
    "\n",
    "# This scales the data to between [0,1]\n",
    "def format_spike_mean_data(spectral_list, time_batch, freq_window):\n",
    "#{\n",
    "    m = len(spectral_list)\n",
    "    fft_imin = freq_window[0]\n",
    "    fft_imax = freq_window[1]\n",
    "    X = np.zeros((m, 2, time_batch))\n",
    "    \n",
    "    mean_power = [np.mean(spectrum[fft_imin:fft_imax]) for spectrum in spectral_list]\n",
    "    power_output = [np.sum(spectrum[fft_imin:fft_imax]) for spectrum in spectral_list]\n",
    "    \n",
    "    # Concatenate power spikes with average power output\n",
    "    mean_power = np.reshape(mean_power / np.std(mean_power), (-1,1))\n",
    "    power_spikes = np.reshape(find_power_spikes(power_output), (-1,1))    \n",
    "    spike_mean = np.concatenate((power_spikes, mean_power), axis=1)\n",
    "\n",
    "    # Zero pad a time_batch at the end\n",
    "    zpad = np.zeros((time_batch, spike_mean.shape[1]))\n",
    "    spike_mean = np.concatenate((spike_mean, zpad), axis=0)\n",
    "    \n",
    "    for i in range(m): X[i,:,:] = spike_mean[i:i+time_batch, :].transpose()\n",
    "    \n",
    "    return X\n",
    "#}\n",
    "\n",
    "X_mean_train = format_spike_mean_data(spectral_train, time_batch, (fft_imin_train, fft_imax_train))\n",
    "X_mean_val = format_spike_mean_data(spectral_val, time_batch, (fft_imin_val, fft_imax_val))\n",
    "print(f\"Input data shape, train: {X_mean_train.shape}, validate: {X_mean_val.shape}\")\n",
    "print(f\"Output data shape, train: {Y_train.shape}, validate: {Y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_power_spike_labels(spectral_list, freq_window):\n",
    "#{\n",
    "    m = len(spectral_list)\n",
    "    fft_imin = freq_window[0]\n",
    "    fft_imax = freq_window[1]\n",
    "    Y = np.zeros((m, 1))\n",
    "    \n",
    "    power_output = [np.sum(spectrum[fft_imin:fft_imax]) for spectrum in spectral_list]\n",
    "    Y[:,0] = find_power_spikes(power_output)\n",
    "    \n",
    "    return Y\n",
    "#}\n",
    "\n",
    "Y_train = generate_power_spike_labels(spectral_train, (fft_imin_train, fft_imax_train))\n",
    "Y_val = generate_power_spike_labels(spectral_val, (fft_imin_val, fft_imax_val))\n",
    "print(f\"Output data shape, train: {Y_train.shape}, validate: {Y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.backend import squeeze\n",
    "from keras.layers import Input, Conv1D, Conv2D, Activation, MaxPooling2D, GRU, Dropout\n",
    "from keras.layers import TimeDistributed, Flatten, Dense, BatchNormalization, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_network(n_freq):\n",
    "#{\n",
    "    # Layer #0: Input\n",
    "    X_input = Input(shape=(n_freq, 1))\n",
    "    \n",
    "    # Layer #1: Conv1D to reduce huge input dim\n",
    "    X = Conv1D(128, kernel_size=16, strides=4)(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    #X = Dropout(0.8)(X)\n",
    "    \n",
    "    # Layer #2: Dense layer\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    #X = Dropout(0.8)(X)\n",
    "\n",
    "    # Layer #3: Dense layer\n",
    "    X = Dense(1, activation='relu')(X)\n",
    "    #X = Dropout(0.8)(X)\n",
    "\n",
    "    # Layer #4: Collapse to 1D, then Binary sigmoid\n",
    "    X = Flatten()(X)\n",
    "    X_output = Dense(1, activation = \"sigmoid\")(X) \n",
    "    \n",
    "    model = keras.models.Model(inputs = X_input, outputs = X_output)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model and configure for training\n",
    "model = create_dense_network(n_freq)\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "early_stop = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(X_train, Y_train, batch_size=m, validation_data=(X_val, Y_val), callbacks=early_stop, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# Plot with respect to accuracy\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')\n",
    "\n",
    "# Plot with respect to loss\n",
    "plt.figure(2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "output = model.predict(X_train)\n",
    "output = np.round(output)\n",
    "\n",
    "if output.shape != Y_train.shape:\n",
    "    output = np.reshape(output, Y_train.shape)\n",
    "\n",
    "# print(np.concatenate((output, Y_train, np.equal(Y_train, output)), axis=1))\n",
    "\n",
    "total_pred_correct = np.sum(np.equal(Y_train, output))\n",
    "total_pred_positive = np.sum(np.logical_and(output, 1))\n",
    "true_positive = np.sum(np.logical_and(Y_train, output))\n",
    "false_negative = np.sum(np.logical_and(np.logical_xor(Y_train, output), Y_train))\n",
    "\n",
    "print(\"Model prediction accuracy:\", total_pred_correct/m )\n",
    "print(\"Model prediction precision:\", true_positive/total_pred_positive )\n",
    "print(\"Model prediction recall:\", true_positive/(true_positive + false_negative), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_network_tbatch(n_freq, time_batch):\n",
    "#{\n",
    "    # Layer #0: Input\n",
    "    X_input = Input(shape=(n_freq, time_batch))\n",
    "    \n",
    "    # Layer #1: Conv1D to reduce huge input dim\n",
    "    X = Conv1D(128, kernel_size=min(n_freq, 16), strides=4)(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    \n",
    "    # Layer #2: Dense layer\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "\n",
    "    # Layer #3: Dense layer\n",
    "    X = Dense(1, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "\n",
    "    # Layer #4: Collapse to 1D, then Binary sigmoid\n",
    "    X = Flatten()(X)\n",
    "    X_output = Dense(1, activation = \"sigmoid\")(X) \n",
    "    \n",
    "    model = keras.models.Model(inputs = X_input, outputs = X_output)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model and configure for training\n",
    "model = create_dense_network_tbatch(2, time_batch)\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "early_stop = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_mean_train, Y_train, batch_size=m, validation_data=(X_mean_val, Y_val), callbacks=early_stop, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# Plot with respect to accuracy\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')\n",
    "\n",
    "# Plot with respect to loss\n",
    "plt.figure(2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape FFT data for time batched SequenceNet\n",
    "time_batch = 10  # ~3 second time window (10 == 307.46ms)\n",
    "X_train = FACTORY.format_spectral_data(spectral_train, time_batch, (fft_imin_train, fft_imax_train))\n",
    "X_val = FACTORY.format_spectral_data(spectral_val, time_batch, (fft_imin_val, fft_imax_val))\n",
    "print(f\"Input data shape, train: {X_train.shape}, validate: {X_val.shape}\")\n",
    "\n",
    "# Y_train = generate_contiguous_labels(midi_train, m, sec_per_step_train)\n",
    "# Y_val = generate_contiguous_labels(midi_val, m, sec_per_step_val)\n",
    "print(f\"Output data shape, train: {Y_train.shape}, validate: {Y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequential_network(n_freq, time_batch):\n",
    "#{\n",
    "    # Layer #0: Input\n",
    "    X_input = Input(shape=(n_freq, time_batch))\n",
    "    \n",
    "    # Layer #1: First GRU layer\n",
    "    X = Bidirectional(GRU(10, return_sequences = True))(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    #X = Dropout(0.8)(X)\n",
    "\n",
    "    # Layer #2: Second GRU layer\n",
    "    X = Bidirectional(GRU(10, return_sequences = False))(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    #X = Dropout(0.8)(X)\n",
    "    \n",
    "    # Layer #3: Collapse to 1D, then Binary sigmoid\n",
    "    #X = Flatten()(X)\n",
    "    X_output = Dense(1, activation = \"sigmoid\")(X) \n",
    "    \n",
    "    # Layer #4: Time-distributed dense layer, binary out\n",
    "    #X_output = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X)\n",
    "    \n",
    "    model = keras.models.Model(inputs = X_input, outputs = X_output)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model and configure for training\n",
    "model = create_sequential_network(n_freq, time_batch)\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "early_stop = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=5, validation_data=(X_val, Y_val), callbacks=early_stop, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# Plot with respect to accuracy\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')\n",
    "\n",
    "# Plot with respect to loss\n",
    "plt.figure(2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLIP_TOLERANCE = 0.5\n",
    "\n",
    "def output_midi_file(nn_output, filename, sec_per_step):\n",
    "#{\n",
    "    midi_orig = MidiFile(re.sub(r'midi_output', 'midi_cleaned', filename))\n",
    "    \n",
    "    tempo = mido.bpm2tempo(DEFAULT_BPM)\n",
    "    ppqn = midi_orig.ticks_per_beat\n",
    "    \n",
    "    midi_file_out = mido.MidiFile(ticks_per_beat=ppqn)\n",
    "    track_compare = midi_orig.tracks[0]\n",
    "    track_out = mido.MidiTrack()\n",
    "    \n",
    "    accrued_ticks = 0\n",
    "    last_strike_tick = 0\n",
    "    for time_step in range(nn_output.shape[0]):\n",
    "    #{\n",
    "        tick = mido.second2tick(time_step * sec_per_step, ppqn, tempo)\n",
    "        if (nn_output[time_step] == 1) and (last_strike_tick < tick):\n",
    "        #{\n",
    "            track_out.append(mido.Message('note_on', note=60, velocity=64, time=int(tick-last_strike_tick)))\n",
    "            track_out.append(mido.Message('note_on', note=60, velocity=0, time=200))\n",
    "            last_strike_tick = int(tick) + 200\n",
    "        #}\n",
    "    #}\n",
    "        \n",
    "    track_out.append(mido.MetaMessage('end_of_track'))\n",
    "    midi_file_out.tracks.append(track_compare)\n",
    "    midi_file_out.tracks.append(track_out)\n",
    "    midi_file_out.save(filename)\n",
    "    \n",
    "    accrued_ticks = 0\n",
    "    for message in midi_file_out.tracks[1]: accrued_ticks += message.time\n",
    "    seconds = mido.tick2second(accrued_ticks, ppqn, tempo)\n",
    "    \n",
    "    print(\"Saved to MIDI file:\", filename)\n",
    "    print(\"  Number of ticks per beat (PPQN):\", ppqn)\n",
    "    print(\"  Number of messages:\", len(midi_file_out.tracks[0]))\n",
    "    print(\"  Number of seconds of messages:\", seconds, \"\\n\")\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, Y):\n",
    "#{\n",
    "    # Evaluate model\n",
    "    output = model.predict(X)\n",
    "    output = np.round(output)\n",
    "    \n",
    "    if output.shape != Y.shape:\n",
    "        output = np.reshape(output, Y.shape)\n",
    "\n",
    "    total_pred_correct = np.sum(np.equal(Y, output))\n",
    "    total_pred_positive = np.sum(np.logical_and(output, 1))\n",
    "    true_positive = np.sum(np.logical_and(Y, output))\n",
    "    false_negative = np.sum(np.logical_and(np.logical_xor(Y, output), Y))\n",
    "\n",
    "    print(\"Model prediction accuracy:\", total_pred_correct/m )\n",
    "    print(\"Model prediction precision:\", true_positive/total_pred_positive )\n",
    "    print(\"Model prediction recall:\", true_positive/(true_positive + false_negative), \"\\n\")\n",
    "    \n",
    "    return(output)\n",
    "#}\n",
    "\n",
    "print(\"Training set:\")\n",
    "output_train = evaluate_model(model, X_train, Y_train)\n",
    "\n",
    "print(\"Validation set:\")\n",
    "output_val = evaluate_model(model, X_val, Y_val)\n",
    "\n",
    "# Generate an actual MIDI file from the model to compare visually with original\n",
    "#output_midi_file(output_train, 'data/midi_output/88_Key_Ascending_Chromatic_Scale.mid', sec_per_step_train)\n",
    "#output_midi_file(output_val, 'data/midi_output/88_Key_Descending_Chromatic_Scale.mid', sec_per_step_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to non-ML estimation using just power_output maxima to estimate note_strikes\n",
    "power_output = [np.sum(spectrum[fft_imin_train:fft_imax_train]) for spectrum in spectral_train]\n",
    "power_spikes = np.reshape(find_power_spikes(power_output), (-1,1))    \n",
    "\n",
    "total_pred_correct = np.sum(np.equal(Y_train, power_spikes))\n",
    "total_pred_positive = np.sum(np.logical_and(power_spikes, 1))\n",
    "true_positive = np.sum(np.logical_and(Y_train, power_spikes))\n",
    "false_negative = np.sum(np.logical_and(np.logical_xor(Y_train, power_spikes), Y_train))\n",
    "\n",
    "print(\"Naive power-max model prediction accuracy:\", total_pred_correct/m )\n",
    "print(\"Naive power-max model prediction precision:\", true_positive/total_pred_positive )\n",
    "print(\"Naive power-max model prediction recall:\", true_positive/(true_positive + false_negative), \"\\n\")\n",
    "\n",
    "# Generate the MIDI file from the model to compare visually with original\n",
    "output_midi_file(power_spikes, '../data/midi_output/88_Key_Ascending_Chromatic_Scale.mid', sec_per_step_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
