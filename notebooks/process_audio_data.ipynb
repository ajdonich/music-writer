{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# This for managing relative imports from nb\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # To supress WavFileWarning: Chunk (non-data) not understood, skipping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_BPM = 120\n",
    "TIME_WINDOW_SEC = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(wav_filename, time_window=TIME_WINDOW_SEC):\n",
    "#{\n",
    "    rate, data = wavfile.read(wav_filename)    \n",
    "    audio_sec = data.shape[0] / rate\n",
    "    \n",
    "    print(f\"Contents of audio file: {wav_filename}:\")\n",
    "    print(\"  Length of data:\", audio_sec, \"seconds\")\n",
    "    print(\"  Sample rate:\", rate, \"Hz\")\n",
    "    print(\"  Number of channels:\", data.shape[1])\n",
    "    print(\"  Samples per channel:\", data.shape[0])\n",
    "    \n",
    "    # Zero-pad end of data to assure all sample are length TIME_WINDOW_SEC\n",
    "    p = round((time_window - ((data.shape[0] / rate) % time_window)) * rate)\n",
    "    data = np.concatenate((data, np.zeros((p, 2))))\n",
    "\n",
    "    audio_sec = data.shape[0] / rate\n",
    "    wav_step = int(time_window * rate)\n",
    "    \n",
    "    print(f\"\\nContents after padding for {time_window} sec window discritization:\")\n",
    "    print(\"  Length of data:\", audio_sec, \"seconds\")\n",
    "    print(\"  Samples per channel:\", data.shape[0])\n",
    "    \n",
    "    # Note: wav data is in stereo, but for now, just using the first channel\n",
    "    spectral = [signal.spectrogram(data[i:i+wav_step, 0], fs=1000, noverlap=128, nfft=256) \\\n",
    "                for i in range(0, data.shape[0], wav_step)]\n",
    "    \n",
    "    # Plot spectogram of a sample for good measure\n",
    "    end_pt = int(len(spectral) - 1) * wav_step\n",
    "    mid_pt = int(len(spectral) / 2) * wav_step\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.title(f\"Spectogram of sample {int(len(spectral) / 2)} length {time_window} sec \")\n",
    "    plt.specgram(data[mid_pt:mid_pt+wav_step, 0], NFFT=256, Fs=1000, noverlap=128)\n",
    "#     plt.specgram(data[0:0+wav_step, 0], NFFT=256, Fs=1000, noverlap=128)\n",
    "#     plt.specgram(data[end_pt:end_pt+wav_step, 0], NFFT=256, Fs=1000, noverlap=128)\n",
    "    \n",
    "    f, t, pxx = spectral[0]\n",
    "    n_freq = pxx.shape[0]                 # Number of frequencies per sample\n",
    "    Tx = pxx.shape[1]                     # Number of time steps per sample\n",
    "    m = len(spectral)                     # Number of TIME_WINDOW_SEC length samples\n",
    "       \n",
    "    # Write into network input form\n",
    "    X = np.zeros((m, Tx, n_freq))\n",
    "    for m_ind in range(m): X[m_ind,:,:] = np.transpose(spectral[m_ind][2])\n",
    "    \n",
    "    print(\"\\nDerived spectral dataset:\")\n",
    "    print(\"  X dataset shape:\", X.shape)\n",
    "    print(\"  Number of frequencies n_freq:\", n_freq)\n",
    "    print(\"  Number of time steps per window Tx:\", Tx)\n",
    "    print(\"  Number of windowed samples m:\", m, \"\\n\")\n",
    "    \n",
    "    return X\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "from mido import MidiFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tick2bin(tick, ppqn, Ty, time_window):\n",
    "#{ \n",
    "    # Assumes 120 BPM = 500000 microseconds per beat (quarter note)\n",
    "    return int((Ty/time_window) * mido.tick2second(tick, ppqn, tempo=500000))\n",
    "#}\n",
    "\n",
    "def bin2tick(time_bin, ppqn, Ty, time_window):\n",
    "#{ \n",
    "    # Assumes 120 BPM = 500000 microseconds per beat (quarter note)\n",
    "    return int(mido.second2tick(time_bin * (time_window/Ty), ppqn, tempo=500000))\n",
    "#}\n",
    "\n",
    "def note2index(note) : return (note - 21)\n",
    "def index2note(index): return (index + 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape2samples(contiguous_data, time_step, dim_tone):\n",
    "#{\n",
    "    \"\"\"\n",
    "    Cuts a contiguous, \"full song\" of data\n",
    "    into a sequence of time_step samples\n",
    "    \n",
    "    time_step = likely Tx (5511) or Ty (1375)\n",
    "    dim_tone = likely n_freq (101) or n_notes (88)\n",
    "    \"\"\"\n",
    "\n",
    "    # Somewhat tricky little reshape maneuver\n",
    "    T1 = np.transpose(contiguous_data)\n",
    "    return np.reshape(T1,  (-1, time_step, dim_tone))\n",
    "    #return np.transpose(R, axes=(0,2,1))\n",
    "#}\n",
    "\n",
    "# Just a quick demonstration of this fcn\n",
    "# A = np.array([[1,2,3,4,5,6,-6, -5,-4,-3,-2,-1],\n",
    "#               [7,8,9,10,11,12,-12,-11,-10,-9,-8,-7], \n",
    "#               [13,14,15,16,17,18,-18,-17,-16,-15,-14,-13]])\n",
    "\n",
    "# print(\"A_orig =\\n\", A, '\\n\\nA_reshaped =\\n', reshape2samples(A, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_label_data(midi_file, Ty, time_window, Y_full):\n",
    "#{\n",
    "    \"\"\"\n",
    "    Parses midi_file data and writes ground-truth\n",
    "    note labels into a time scaled data-block Y\n",
    "    \n",
    "    midi_file = the single-song MIDI to parse\n",
    "    Y = the time-scaled block of ground-truth labels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    on_notes = {}\n",
    "    accrued_ticks = 0\n",
    "    ppqn = midi_file.ticks_per_beat\n",
    "    \n",
    "    for message in midi_file.tracks[0]:\n",
    "    #{\n",
    "        accrued_ticks += message.time\n",
    "        if message.type == 'note_on':\n",
    "        #{\n",
    "            # Non-zero velocity --> note has been struck\n",
    "            # Zero velocity --> note has been released\n",
    "\n",
    "            if message.velocity > 0: \n",
    "                on_notes[message.note] = accrued_ticks\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Scale tick-window to spectral-bin-window\n",
    "                start = tick2bin(on_notes[message.note], ppqn, Ty, time_window)\n",
    "                end   = tick2bin(accrued_ticks, ppqn, Ty, time_window)\n",
    "                \n",
    "                # Valid MIDI-note codes are: (21-108)\n",
    "                # Scale them to output node index: (0-88)\n",
    "                Y_full[note2index(message.note), start:end] = 1\n",
    "        #}\n",
    "    #}\n",
    "    return reshape2samples(Y_full, Ty, n_tones)\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label_data(midi_filename, m, n_tones, Ty, time_window=TIME_WINDOW_SEC):\n",
    "#{\n",
    "    mid = MidiFile(midi_filename)\n",
    "    tempo = mido.bpm2tempo(DEFAULT_BPM)\n",
    "    \n",
    "    assert len(mid.tracks) == 1, f\"MIDI file contains invalid number of tracks: {len(mid.tracks)}\"\n",
    " \n",
    "    accrued_ticks = 0\n",
    "    for message in mid.tracks[0]: accrued_ticks += message.time\n",
    "    seconds = mido.tick2second(accrued_ticks, mid.ticks_per_beat, tempo)\n",
    "    \n",
    "    print(\"Output paramaters defined as:\")\n",
    "    print(\"  Number of output nodes n_tones:\", n_tones)\n",
    "    print(\"  Number of time steps per output window Ty:\", Ty, \"\\n\")\n",
    "\n",
    "    print(\"Contents of MIDI file:\", midi_filename)\n",
    "    print(\"  Tempo:\", DEFAULT_BPM, \"BPM or\", tempo, \"micros/beat\")\n",
    "    print(\"  Number of ticks per beat (PPQN):\", mid.ticks_per_beat)\n",
    "    print(\"  Number of messages:\", len(mid.tracks[0]))\n",
    "    print(\"  Number of seconds of messages:\", seconds)\n",
    "\n",
    "    Y_zfull = np.zeros((n_tones, m * Ty))\n",
    "    Y = format_label_data(mid, Ty, time_window, Y_zfull)                  \n",
    "    \n",
    "    # Plot lables\n",
    "    mid_pt = int(m / 2)\n",
    "    plt.figure(2)\n",
    "    plt.title(f\"Label data sample {mid_pt} length {time_window} sec \")\n",
    "    plt.pcolormesh(np.transpose(Y[mid_pt,:,:]))\n",
    "\n",
    "    print(f\"\\nDiscretized MIDI lables into {time_window} sec window samples:\")\n",
    "    print(\"  Y label dataset shape:\", Y.shape, \"\\n\")\n",
    "        \n",
    "    return Y\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TRAINING data dataset\n",
    "X_train = generate_dataset('../data/audio/88_Key_Ascending_Chromatic_Scale.wav', TIME_WINDOW_SEC)\n",
    "\n",
    "# Network inputs\n",
    "m = X_train.shape[0]                  # Number of TIME_WINDOW_SEC length samples\n",
    "Tx = X_train.shape[1]                 # Number of time steps per sample\n",
    "n_freq = X_train.shape[2]             # Number of frequencies per sample\n",
    "\n",
    "# Network ouput (defined by choice)\n",
    "n_tones = 88                          # Number of output nodes == 88 on/off piano keys\n",
    "Ty = min(Tx, 1375)                    # Number of time steps per TIME_WINDOW_SEC length output sample\n",
    "\n",
    "# Generate matching label dataset\n",
    "Y_train = generate_label_data('../data/midi_cleaned/88_Key_Ascending_Chromatic_Scale.mid', m, n_tones, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate VALIDATION datasets\n",
    "X_val = generate_dataset('../data/audio/88_Key_Descending_Chromatic_Scale.wav', TIME_WINDOW_SEC)\n",
    "Y_val = generate_label_data('../data/midi_cleaned/88_Key_Descending_Chromatic_Scale.mid', m, n_tones, Ty)\n",
    "\n",
    "print(\"Validating on:\", X_val.shape[0], \"samples\")\n",
    "assert (Tx == X_val.shape[1]) and (n_freq == X_val.shape[2]), \\\n",
    "    f\"Validation data shape {X_val.shape} does not match training {X_train.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Conv1D, BatchNormalization, Activation, Dropout, GRU, TimeDistributed, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras model\n",
    "\n",
    "def generate_model(Tx, n_freq, Ty, n_tones):\n",
    "#{\n",
    "    X_input = Input(shape=(Tx, n_freq))\n",
    "\n",
    "    # Step 1: CONV layer (≈4 lines)\n",
    "    # X = Conv1D(196, kernel_size=15, strides=4)(X_conv_in) # CONV1D\n",
    "    # X = BatchNormalization()(X)                           # Batch normalization\n",
    "    # X = Activation('relu')(X)                             # ReLu activation\n",
    "    # X = Dropout(0.2)(X)                                   # dropout (use 0.2)\n",
    "\n",
    "    # Step 2: First GRU Layer (≈4 lines)\n",
    "    X = GRU(units = 128, return_sequences = True)(X_input) # GRU (use 128 units and return the sequences)\n",
    "    X = Dropout(0.2)(X)                                     # dropout (use 0.2)\n",
    "    X = BatchNormalization()(X)                             # Batch normalization\n",
    "\n",
    "    # Step 3: Second GRU Layer (≈4 lines)\n",
    "    X = GRU(units = 128, return_sequences = True)(X)    # GRU (use 128 units and return the sequences)\n",
    "    X = Dropout(0.2)(X)                                 # dropout (use 0.2)\n",
    "    X = BatchNormalization()(X)                         # Batch normalization\n",
    "    X = Dropout(0.2)(X)                                 # dropout (use 0.2)\n",
    "\n",
    "    # Step 4: Time-distributed dense layer (≈1 line)\n",
    "    X = TimeDistributed(Dense(n_tones, activation = \"sigmoid\"))(X) # time distributed  (sigmoid)\n",
    "\n",
    "    model = keras.models.Model(inputs = X_input, outputs = X)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model and configure for training\n",
    "model = generate_model(Tx, n_freq, Ty, n_tones)\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "early_stop = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(X_train, Y_train, batch_size = m, validation_data=(X_val, Y_val), callbacks=early_stop, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../data/h5/music_model_19_02_08.h5') \n",
    "\n",
    "# List all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# Plot with respect to accuracy\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')\n",
    "\n",
    "# Plot with respect to loss\n",
    "plt.figure(2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "\n",
    "FLIP_TOLERANCE = 0.5\n",
    "\n",
    "def output_midi_file(nn_output, filename, ppqn, Ty):\n",
    "#{\n",
    "    midi_file_out = mido.MidiFile(ticks_per_beat=ppqn)\n",
    "    track_out = mido.MidiTrack()\n",
    "    \n",
    "    last_delta = 0\n",
    "    accrued_steps = 0\n",
    "    for sample in range(nn_output.shape[0]):\n",
    "    #{\n",
    "        step_flips = []\n",
    "        for step in range(Ty-1):\n",
    "        #{\n",
    "            accrued_steps += 1\n",
    "            total_ticks = bin2tick(accrued_steps, ppqn, Ty, TIME_WINDOW_SEC)\n",
    "            \n",
    "            # Look for note indices that flip on/off between steps\n",
    "            delta = nn_output[sample, step+1, :] - nn_output[sample, step, :]\n",
    "            on_idx = np.where(delta > FLIP_TOLERANCE)[0]\n",
    "            off_idx = np.where(delta < -FLIP_TOLERANCE)[0]\n",
    "            \n",
    "            for on in on_idx:\n",
    "                step_flips.append(mido.Message('note_on', note=index2note(on), velocity=64))\n",
    "                \n",
    "            for off in off_idx:\n",
    "                step_flips.append(mido.Message('note_on', note=index2note(off), velocity=0))\n",
    "            \n",
    "            if len(step_flips) > 0:\n",
    "            #{\n",
    "                # Only first message/step can have non-zero relative time      \n",
    "                step_flips[0].time = total_ticks - last_delta\n",
    "                last_delta = total_ticks\n",
    "                track_out.extend(step_flips)\n",
    "            #}\n",
    "        #}\n",
    "    #}\n",
    "    \n",
    "    track_out.append(mido.MetaMessage('end_of_track'))\n",
    "    midi_file_out.tracks.append(track_out)     \n",
    "    midi_file_out.save(filename)\n",
    "    \n",
    "    accrued_ticks = 0\n",
    "    tempo = mido.bpm2tempo(DEFAULT_BPM)\n",
    "    for message in midi_file_out.tracks[0]: accrued_ticks += message.time\n",
    "    seconds = mido.tick2second(accrued_ticks, midi_file_out.ticks_per_beat, tempo)\n",
    "    \n",
    "    print(\"Saved to MIDI file:\", filename)\n",
    "    print(\"  Number of ticks per beat (PPQN):\", ppqn)\n",
    "    print(\"  Number of messages:\", len(midi_file_out.tracks[0]))\n",
    "    print(\"  Number of seconds of messages:\", seconds)\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(X_train)\n",
    "ppqn = MidiFile('../data/midi_cleaned/88_Key_Ascending_Chromatic_Scale.mid').ticks_per_beat\n",
    "output_midi_file(output, '../data/midi_output/88_Key_Ascending_Chromatic_Scale.mid', ppqn, Ty)\n",
    "\n",
    "# output1 = model.predict(X_train[0:25,:,:])\n",
    "# output2 = model.predict(X_train[25:,:,:])\n",
    "\n",
    "# ppqn1 = MidiFile('data/midi_cleaned/Cry_me_a_river_simple.mid').ticks_per_beat\n",
    "# ppqn2 = MidiFile('data/midi_cleaned/A_fine_romance_simple.mid').ticks_per_beat\n",
    "\n",
    "# output_midi_file(output1, 'data/midi_output/Cry_me_a_river_output.mid', ppqn1)\n",
    "# output_midi_file(output1, 'data/midi_output/A_fine_romance_output.mid', ppqn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
