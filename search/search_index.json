{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Piano Music Transcription Project Note: this project is a work in progress, still very early stage and not cleaned up for easily understandable presentation. This project was inspired by the amazing piano improvisations of jazz pianist Brad Mehldau. If you've never heard him, here's a few samples: 1) My Favorite Things - Jazz a Vienne , 2) Blackbird - Live in London , 3) Paranoid Android - Transcription , the last of which actually presents a (human) transcription to give you an idea what this project is aiming for. Like most jazz musicians, most of Mehldau's recordings are live performances, each uniquely improvised on the spot. These rarely get transcribed because of the difficulty of such a task for even a single song performance. Harmony in particular (probably by ear-candy-design, in a manner of speaking) exibits a rich and ever variable mixture of constructive and destructive wave phenomenon and is thus very tricky to reverse engineer into written music. The goal of this project then is transcription of solo piano recordings, or more precisely to create simplified MIDI files given WAV audio (there are existing tools to translate MIDI into musical notation, which is relatively straightforward by comparison). Though automated musical transcription has received a lot of attention, the programs I could find produced essentially gibberish given a solo Meldau recording. Granted they were free versions, but like most of my projects recently, I'm as focused on curious exploration of ML/Deep Learning/AI as I am on pragmatic end products, plus I know this problem is far from solved in a general. Introductory Progress Thematically my focus thus far has been 1) general MIDI file handling, 2) translation of audio signals into the frequency domain, and 3) processing spectral data with NNs. This strategy (i.e. audio -> spectral -> network -> midi) seemed to be both the most logical approach and a relatively common one, based on my research of other transcription projects. 1 I've employed an open source package: Mido for MIDI file handling, and I'm using SciPy libraries for the spectral translation (Wav file handling and DFTs). I've experimented with a variety of NN architectures so far, primarily of GRU and CNN-to-Dense varieties. To conduct supervised learning, I've restricted myself to training with audio files that I have corresponding MIDI data for. I also created a very stripped-down MIDI/audio file set of simple ascending and descending chromatic scales. I was surprised to find even this very simple scale test to be non-trivial for a network to predict with perfect accuracy. I was intrigued to notice that a typical mean-squared-error measurement can be minimized to a high degree of accuracy, but still sound obviously wrong to our ears rhythmically, by which I mean, I believe our minds are judging rhythmic accuracy differently. For example, a drum beat that is consistently late every beat by a half-second sounds more \"correct\" to us than a drum that is spot on most of the time but erratically drops out without a clear pattern, even though this latter case may be more mathematically accurate according to the MSE measurement). Exploring the Repository Please feel free to run through the notebooks. These should all execute but (my apologies) they are not well commented or organized at this point, many still with draft versions of functions etc. Again, the project is in early stages and has not be manicured for presentation. Generally, you can find .py files in the mwriter directory and .ipynb files in the notebooks directory. The sandbox directory is just very rough, scratch pad stuff at this point. Footnotes: 1 See: Automatic Music Transcription: An Overview","title":"Home"},{"location":"#piano-music-transcription-project","text":"Note: this project is a work in progress, still very early stage and not cleaned up for easily understandable presentation. This project was inspired by the amazing piano improvisations of jazz pianist Brad Mehldau. If you've never heard him, here's a few samples: 1) My Favorite Things - Jazz a Vienne , 2) Blackbird - Live in London , 3) Paranoid Android - Transcription , the last of which actually presents a (human) transcription to give you an idea what this project is aiming for. Like most jazz musicians, most of Mehldau's recordings are live performances, each uniquely improvised on the spot. These rarely get transcribed because of the difficulty of such a task for even a single song performance. Harmony in particular (probably by ear-candy-design, in a manner of speaking) exibits a rich and ever variable mixture of constructive and destructive wave phenomenon and is thus very tricky to reverse engineer into written music. The goal of this project then is transcription of solo piano recordings, or more precisely to create simplified MIDI files given WAV audio (there are existing tools to translate MIDI into musical notation, which is relatively straightforward by comparison). Though automated musical transcription has received a lot of attention, the programs I could find produced essentially gibberish given a solo Meldau recording. Granted they were free versions, but like most of my projects recently, I'm as focused on curious exploration of ML/Deep Learning/AI as I am on pragmatic end products, plus I know this problem is far from solved in a general.","title":"Piano Music Transcription Project"},{"location":"#introductory-progress","text":"Thematically my focus thus far has been 1) general MIDI file handling, 2) translation of audio signals into the frequency domain, and 3) processing spectral data with NNs. This strategy (i.e. audio -> spectral -> network -> midi) seemed to be both the most logical approach and a relatively common one, based on my research of other transcription projects. 1 I've employed an open source package: Mido for MIDI file handling, and I'm using SciPy libraries for the spectral translation (Wav file handling and DFTs). I've experimented with a variety of NN architectures so far, primarily of GRU and CNN-to-Dense varieties. To conduct supervised learning, I've restricted myself to training with audio files that I have corresponding MIDI data for. I also created a very stripped-down MIDI/audio file set of simple ascending and descending chromatic scales. I was surprised to find even this very simple scale test to be non-trivial for a network to predict with perfect accuracy. I was intrigued to notice that a typical mean-squared-error measurement can be minimized to a high degree of accuracy, but still sound obviously wrong to our ears rhythmically, by which I mean, I believe our minds are judging rhythmic accuracy differently. For example, a drum beat that is consistently late every beat by a half-second sounds more \"correct\" to us than a drum that is spot on most of the time but erratically drops out without a clear pattern, even though this latter case may be more mathematically accurate according to the MSE measurement).","title":"Introductory Progress"},{"location":"#exploring-the-repository","text":"Please feel free to run through the notebooks. These should all execute but (my apologies) they are not well commented or organized at this point, many still with draft versions of functions etc. Again, the project is in early stages and has not be manicured for presentation. Generally, you can find .py files in the mwriter directory and .ipynb files in the notebooks directory. The sandbox directory is just very rough, scratch pad stuff at this point.","title":"Exploring the Repository"},{"location":"#footnotes","text":"1 See: Automatic Music Transcription: An Overview","title":"Footnotes:"},{"location":"quickstart/","text":"Project: music-writer Note: this project is a work in progress, still very early stage and not cleaned up for easily understandable presentation. The goal of this project is automated transcription of solo piano recordings, or more precisely to create simplified MIDI files given WAV audio (as there are existing tools to translate MIDI into musical notation). This project was inspired by the improvisations of jazz pianist Brad Mehldau, and though automated musical transcription has received a lot of attention, the programs I could find produced essentially gibberish given Meldau recordings. Please see the project blog for a more detailed description. Installation: The repository is setup for pipenv configuration management, thus you'll find a Pipfile rather than requirements.txt file. You may access installation instructions for pipenv here: Pipenv Installation Instructions Once pipenv has been successfully installed, the following commands may be executes to install the rubiks-cube project: $ git clone https://github.com/ajdonich/music-writer.git $ cd music-writer $ pipenv install $ pipenv --dev install Execution: Again, the project is in early stages and has not be manicured for presentation, however, please feel free to run through the notebooks. These should all execute but (my apologies) they are not well commented or organized at this point, many still with draft versions of functions etc. Generally, you can find .py files in the mwriter directory and .ipynb files in the notebooks directory. The sandbox directory is just very rough, scratch pad stuff at this point. To run IPython/Jupyter notebooks, assure you are in the music-writer root directory, then launch a notebook session from the command line using: $ pipenv shell $ jupyter lab Then from within the notebook session that is launched in your browser, navigate to the notebooks directory. This directory contains the following set of .ipynb files that you're free to run in whatever order you prefer: data_generation.ipynb learn_fft.ipynb neural_fft.ipynb process_audio_data.ipynb They explore training a variety of NN architectures, pre-processing several audio files (with conversions to spectral formats) and MIDI file handing (though to really look at MIDI files, you'll need to open with a DAW such as Garage Band or otherwise).","title":"Quick Start"},{"location":"quickstart/#project-music-writer","text":"Note: this project is a work in progress, still very early stage and not cleaned up for easily understandable presentation. The goal of this project is automated transcription of solo piano recordings, or more precisely to create simplified MIDI files given WAV audio (as there are existing tools to translate MIDI into musical notation). This project was inspired by the improvisations of jazz pianist Brad Mehldau, and though automated musical transcription has received a lot of attention, the programs I could find produced essentially gibberish given Meldau recordings. Please see the project blog for a more detailed description.","title":"Project: music-writer"},{"location":"quickstart/#installation","text":"The repository is setup for pipenv configuration management, thus you'll find a Pipfile rather than requirements.txt file. You may access installation instructions for pipenv here: Pipenv Installation Instructions Once pipenv has been successfully installed, the following commands may be executes to install the rubiks-cube project: $ git clone https://github.com/ajdonich/music-writer.git $ cd music-writer $ pipenv install $ pipenv --dev install","title":"Installation:"},{"location":"quickstart/#execution","text":"Again, the project is in early stages and has not be manicured for presentation, however, please feel free to run through the notebooks. These should all execute but (my apologies) they are not well commented or organized at this point, many still with draft versions of functions etc. Generally, you can find .py files in the mwriter directory and .ipynb files in the notebooks directory. The sandbox directory is just very rough, scratch pad stuff at this point. To run IPython/Jupyter notebooks, assure you are in the music-writer root directory, then launch a notebook session from the command line using: $ pipenv shell $ jupyter lab Then from within the notebook session that is launched in your browser, navigate to the notebooks directory. This directory contains the following set of .ipynb files that you're free to run in whatever order you prefer: data_generation.ipynb learn_fft.ipynb neural_fft.ipynb process_audio_data.ipynb They explore training a variety of NN architectures, pre-processing several audio files (with conversions to spectral formats) and MIDI file handing (though to really look at MIDI files, you'll need to open with a DAW such as Garage Band or otherwise).","title":"Execution:"}]}